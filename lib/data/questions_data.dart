import '../models/listening_question.dart';

final List<ListeningQuestion> listeningQuestions = [
  const ListeningQuestion(
    audioPath: 'voices/cefr_a10001.wav',
    imagePath: 'images/cefr_a10001.png',
    englishText: 'My name is John and I am from Japan.',
    duration: 4,
  ),
  const ListeningQuestion(
    audioPath: 'voices/cefr_a10002.wav',
    imagePath: 'images/cefr_a10002.png',
    englishText: 'I have a cat and a dog.',
    duration: 3,
  ),
  const ListeningQuestion(
    audioPath: 'voices/cefr_a10003.wav',
    imagePath: 'images/cefr_a10003.png',
    englishText: 'I like to eat and sleep.',
    duration: 3,
  ),
  const ListeningQuestion(
    audioPath: 'voices/cefr_a10004.wav',
    imagePath: 'images/cefr_a10004.png',
    englishText: 'The sky is blue.',
    duration: 2,
  ),
  const ListeningQuestion(
    audioPath: 'voices/cefr_a10005.wav',
    imagePath: 'images/cefr_a10005.png',
    englishText: 'I go to the park every day.',
    duration: 3,
  ),
  const ListeningQuestion(
    audioPath: 'voices/cefr_a10006.wav',
    imagePath: 'images/cefr_a10006.png',
    englishText: 'She is reading a book.',
    duration: 3,
  ),
  const ListeningQuestion(
    audioPath: 'voices/cefr_a10007.wav',
    imagePath: 'images/cefr_a10007.png',
    englishText: 'He drinks hot coffee.',
    duration: 2,
  ),
  const ListeningQuestion(
    audioPath: 'voices/cefr_a10008.wav',
    imagePath: 'images/cefr_a10008.png',
    englishText: 'They are playing in the garden.',
    duration: 3,
  ),
  const ListeningQuestion(
    audioPath: 'voices/cefr_a10009.wav',
    imagePath: 'images/cefr_a10009.png',
    englishText: 'It is a sunny and hot day.',
    duration: 3,
  ),
  const ListeningQuestion(
    audioPath: 'voices/cefr_a10010.wav',
    imagePath: 'images/cefr_a10010.png',
    englishText: 'What time is it now?',
    duration: 2,
  ),
];
